{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro: NLTK Package will be used for basic text processing.It has functions for breaking the text into lines and then further down to words. Below function sent_tokenize will be used to break the given paragraph into lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello guys, welcome to the nltk intro!!.', 'We are going to use nltk for text pre processing.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"\"\"Hello guys, welcome to the nltk intro!!.\n",
    "We are going to use nltk for text pre processing.\"\"\"\n",
    "tokenized_sent_text = sent_tokenize(text)\n",
    "print(tokenized_sent_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokenize will be used to break the sentence into words by using space as seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'How', 'are', 'you', 'doing', 'today', '?', 'Its', 'a', 'good', 'place', ',', 'is', \"n't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text=\"Hello , How are you doing today? Its a good place, isn't it?\"\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words: Below code to get the stopwords for english. Stopwords can be removed as part of text preprocessing. These words are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RaghavaKrishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'does', 'y', 'yourself', 'doing', 'himself', 'just', 'below', 'until', 'ain', 'nor', 'wasn', 'into', \"you've\", 're', 'had', 'most', 'd', 'but', 'under', 'than', 'your', 'because', \"it's\", 'from', \"wasn't\", 'shan', 'shouldn', 'isn', 'a', 'all', 'only', 'is', 'about', 'am', 'again', 'him', 'should', 'this', \"she's\", 'didn', \"you'd\", 'which', 'been', 'few', \"shouldn't\", 'to', 'myself', 'hasn', \"haven't\", 'any', 'wouldn', 'he', 'herself', \"you're\", 't', 'an', 'why', \"hasn't\", 'be', 'off', 'down', 'her', 'more', 'in', 'too', 'those', 'are', 'with', 'the', 'won', 'me', 'has', 'against', 'when', 'needn', 'while', 've', 'their', \"that'll\", 'of', 'as', \"mightn't\", 'm', 'after', 'themselves', 'ourselves', 'll', 'ours', \"isn't\", 'do', 'each', 'can', 'out', 'whom', 'both', \"should've\", 'during', 'its', 'very', \"couldn't\", 'yours', \"shan't\", \"didn't\", 'once', \"aren't\", 'over', 'for', 'them', 'they', 'by', \"doesn't\", 'she', 'did', \"won't\", 'where', \"wouldn't\", 'further', \"weren't\", \"you'll\", 'what', 'couldn', 'then', 'own', 'mightn', 'our', 'here', 'no', 'o', 'if', 'his', \"don't\", 'now', \"hadn't\", 'before', 'other', 'don', 'haven', \"needn't\", 'so', 'through', 'my', 'were', 'mustn', 'have', 'and', 'there', 'these', 'who', 'it', 'how', 'aren', \"mustn't\", 'such', 'hers', 'that', 'up', 'theirs', 's', 'at', 'will', 'between', 'hadn', 'was', 'having', 'above', 'weren', 'we', 'you', 'yourselves', 'not', 'same', 'doesn', 'ma', 'on', 'i', 'itself', 'being', 'some', 'or'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text=\"\"\"Hello guys, youhave to check yourself being a good programmer\"\"\"\n",
    "tokenized_sent_text = sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sent: ['Hello', ',', 'How', 'are', 'you', 'doing', 'today', '?', 'Its', 'a', 'good', 'place', ',', 'is', \"n't\", 'it', '?']\n",
      "After Stop words removal: ['Hello', ',', 'How', 'today', '?', 'Its', 'good', 'place', ',', \"n't\", '?']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized sent:\",tokenized_word)\n",
    "print(\"After Stop words removal:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RaghavaKrishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tell', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('now', 'RB'),\n",
       " ('!', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('ca', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('bear', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('suspense', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"Tell me now! I can't bear the suspense!\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('bear', 'NN'), ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2=word_tokenize(\"She saw a bear.\")\n",
    "nltk.pos_tag(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\RaghavaKrishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_treebank_pos_tagger') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\RaghavaKrishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\RaghavaKrishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER: Named entity recognition, used to recognize the enttities such as person,date,location, city etc from the given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mike/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  since/IN\n",
      "  2010/CD\n",
      "  in/IN\n",
      "  (ORGANIZATION IBM/NNP)\n",
      "  at/IN\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk,pos_tag\n",
    "sentence = \"Mike and John are working since 2010 in IBM at California.\"\n",
    "entities=ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "print (entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[おげんきですか] /(exp) how are you?/\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(from_lang=\"Japanese\",to_lang=\"English\")\n",
    "translation = translator.translate(\"お元気ですか\")\n",
    "print (translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ja'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "detect(\"お元気ですか\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below SentimentIntensityAnalyzer will be used to get the polarity of a given sentence. Based on the polarity values this sentence can be identified as positive/negative/neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\RaghavaKrishna\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 0.438, 'neg': 0.0, 'compound': 0.5696, 'neu': 0.562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RaghavaKrishna\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid=SentimentIntensityAnalyzer()\n",
    "sentiment = sid.polarity_scores(\"please escalate this issue as this is urgent!!\")\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
